{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"TIKI: Data Compliance and Quality\"\n",
    "author: \"Leona Hammelrath\"\n",
    "date: \"10 April 2024\"\n",
    "toc: true\n",
    "number-sections: true\n",
    "\n",
    "format: html\n",
    "execute:\n",
    "    eval: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "First Data for Passive Data Collection using Smartwatches and GPS from the PREACT Study. \n",
    "\n",
    "## Introduction\n",
    "\n",
    "Treatment personalization is highly discussed to counteract insufficient response rates in psychotherapy. In the quest for criteria allowing informed selection or adaptation, ambulatory assessment data (i.e. EMA, passive sensing)are a key component, as processes happening outside of therapy sessions can be depicted in high temporal and/or spatial resolution.\n",
    "\n",
    "PREACT is a multicenter prospective-longitudinal study investigating different predictors of non-response (i.e. EEG, fMRI) in around 500 patients undergoing cognitive behavioral therapy for internalizing disorders (https://forschungsgruppe5187.de/de). \n",
    "\n",
    "## Methods\n",
    "Patients can enroll for therapy-accompanying ambulatory assessment. They are provided with a customized study app and a state-of-the-art smartwatch collecting passive data like GPS and heart rate for up to 365 days. In parallel, three 14-day EMA phases (pre-, mid- and post-therapy) cover transdiagnostic (i.e. emotion regulation), contextual and therapy-related aspects.  \n",
    "\n",
    "Here, we present first results on data compliance and quality for the passive sensing data as well as EMA assessments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import date, datetime\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "from config import datapath, proj_sheet\n",
    "\n",
    "today = \"22042024\"\n",
    "\n",
    "# passive + ema_data\n",
    "datapath1 = datapath + f\"export_tiki_{today}/\"\n",
    "file_pattern = os.path.join(datapath1, \"epoch_part*.csv\")\n",
    "file_list = glob.glob(file_pattern)\n",
    "file_list.sort()\n",
    "\n",
    "df_complete = pd.concat((pd.read_csv(f, encoding=\"latin-1\", low_memory=False) for f in file_list), ignore_index=True)\n",
    "today = \"15042024\"\n",
    "df_redcap_zert = pd.read_csv(datapath + f\"ZERTIFIZIERUNGFOR518_DATA_{today}.csv\")\n",
    "df_redcap = pd.read_csv(datapath + f\"FOR5187_DATA_{today}.csv\")\n",
    "df_monitoring = pd.read_csv(f\"https://docs.google.com/spreadsheets/d/{proj_sheet}/export?format=csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/leonahammelrath/FU_Psychoinformatik/Github/tiki_code/data//Kosten_2024-04-15.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_redcap_kosten \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(datapath \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Kosten_2024-04-15.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m df_redcap_zert_kosten \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(datapath \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/ZertKosten_2024-04-15.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m df_redcap_kosten \u001b[38;5;241m=\u001b[39m df_redcap_kosten[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosts_ema_min\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosts_ema_burden\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosts_ema_text\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/leonahammelrath/FU_Psychoinformatik/Github/tiki_code/data//Kosten_2024-04-15.csv'"
     ]
    }
   ],
   "source": [
    "df_redcap_kosten = pd.read_csv(datapath + \"Kosten_2024-04-15.csv\")\n",
    "df_redcap_zert_kosten = pd.read_csv(datapath + \"ZertKosten_2024-04-15.csv\")\n",
    "\n",
    "df_redcap_kosten = df_redcap_kosten[['for_id','costs_ema_min', 'costs_ema_burden', 'costs_ema_text']]\n",
    "df_redcap_zert_kosten = df_redcap_zert_kosten[['for_id','costs_ema_min', 'costs_ema_burden', 'costs_ema_text']]\n",
    "\n",
    "df_redcap_kosten = pd.concat([df_redcap_kosten, df_redcap_zert_kosten],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_redcap_kosten.dropna(subset= \"costs_ema_burden\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsi_scales = {\n",
    "    'bsi_somatization': ['bsi_2', 'bsi_7', 'bsi_23', 'bsi_29', 'bsi_30', 'bsi_33', 'bsi_37'],\n",
    "    'bsi_compulsivity': ['bsi_5', 'bsi_15', 'bsi_26', 'bsi_27', 'bsi_32', 'bsi_36'],\n",
    "    'bsi_insecurity': ['bsi_20', 'bsi_21', 'bsi_22', 'bsi_42'],\n",
    "    'bsi_depression': ['bsi_9', 'bsi_16', 'bsi_17', 'bsi_18', 'bsi_35', 'bsi_50'],\n",
    "    'bsi_anxiety': ['bsi_1', 'bsi_12', 'bsi_19', 'bsi_38', 'bsi_45', 'bsi_49'],\n",
    "    'bsi_aggression': ['bsi_6', 'bsi_13', 'bsi_40', 'bsi_41', 'bsi_46'],\n",
    "    'bsi_phobia': ['bsi_8', 'bsi_28', 'bsi_31', 'bsi_43', 'bsi_47'],\n",
    "    'bsi_paranoia': ['bsi_4', 'bsi_10', 'bsi_24', 'bsi_48', 'bsi_51'],\n",
    "    'bsi_psychotizism': ['bsi_3', 'bsi_14', 'bsi_34', 'bsi_44', 'bsi_53'],\n",
    "    'bsi_additional': ['bsi_11', 'bsi_25', 'bsi_39', 'bsi_52']\n",
    "}\n",
    "\n",
    "# Check for missing columns and calculate each scale\n",
    "for scale, columns in bsi_scales.items():\n",
    "    missing_cols = [col for col in columns if col not in df_redcap_zert.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Missing columns in DataFrame for {scale}:\", missing_cols)\n",
    "    else:\n",
    "        # Sum the specified columns and create a new column for each scale\n",
    "        df_redcap_zert[scale] = df_redcap_zert[columns].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsi_columns = [f'bsi_{i}' for i in range(1, 54)]\n",
    "df_redcap_zert['bsi_gs'] = df_redcap_zert[bsi_columns].sum(axis=1)\n",
    "df_redcap_zert['bsi_gsi'] = df_redcap_zert[\"bsi_gs\"]/53\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_redcap_zert = df_redcap_zert[['for_id', 'ema_start_date','ema_smartphone', 'ema_special_event', 'age', 'gender', 'marital_status', 'partnership', 'graduation','profession','years_of_education',\n",
    " 'employability', 'scid_cv_date','ses', 'somatic_problems','scid_cv_prim_cat', 'bsi_date','bsi_somatization',\n",
    " 'bsi_compulsivity','bsi_insecurity','bsi_depression','bsi_anxiety','bsi_aggression','bsi_phobia','bsi_paranoia',\n",
    " 'bsi_psychotizism','bsi_additional','bsi_gs','bsi_gsi', 'prior_treatment']] #ema_wear_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redcap data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_redcap = pd.concat([df_redcap, df_redcap_zert],ignore_index=True)\n",
    "\n",
    "df_redcap = df_redcap[['for_id', 'ema_start_date','ema_smartphone',\n",
    " 'ema_wear_exp', 'ema_special_event', 'age', 'gender', 'marital_status', 'partnership', 'graduation','profession','years_of_education',\n",
    " 'employability','ses', 'somatic_problems','scid_cv_prim_cat','bsi_somatization', 'prior_treatment',\n",
    " 'bsi_compulsivity','bsi_insecurity','bsi_depression','bsi_anxiety','bsi_aggression','bsi_phobia','bsi_paranoia',\n",
    " 'bsi_psychotizism','bsi_additional','bsi_gs','bsi_gsi']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_redcap[\"ema_start_date\"] = pd.to_datetime(df_redcap[\"ema_start_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_mapping = {\n",
    "    1: 'male',\n",
    "    2: 'female',\n",
    "    3: 'diverse',\n",
    "    4: 'no gender',\n",
    "    5: 'not specified'\n",
    "}\n",
    "\n",
    "scid_cv_cat_mapping = {\n",
    "    1: 'Depressive Disorder',\n",
    "    2: 'Specific Phobia',\n",
    "    3: 'Social Anxiety Disorder',\n",
    "    4: 'Agoraphobia and/or Panic Disorder',\n",
    "    5: 'Generalized Anxiety Disorder',\n",
    "    6: 'Obsessive-Compulsive Disorder',\n",
    "    7: 'Post-Traumatic Stress Disorder'\n",
    "}\n",
    "\n",
    "marital_status_mapping = {\n",
    "    1: 'single',\n",
    "    2: 'married/registered partnership',\n",
    "    3: 'divorced',\n",
    "    4: 'separated',\n",
    "    5: 'widowed',\n",
    "    6: 'other'\n",
    "}\n",
    "\n",
    "employability_mapping = {\n",
    "    0: 'employable',\n",
    "    1: 'unemployable (on sick leave)',\n",
    "    2: 'on disability pension',\n",
    "    3: 'on retirement pension',\n",
    "    4: 'other'\n",
    "}\n",
    "\n",
    "graduation_mapping = {\n",
    "    0: 'still in school',\n",
    "    1: 'no school degree',\n",
    "    2: 'elementary school degree or equivalent',\n",
    "    3: 'middle school degree or equivalent',\n",
    "    4: 'high school diploma/university entrance qualification',\n",
    "    5: 'other'\n",
    "}\n",
    "\n",
    "profession_mapping = {\n",
    "    0: 'still in training or studies',\n",
    "    1: 'no training degree',\n",
    "    2: 'vocational training, including technical school',\n",
    "    3: 'university or college degree',\n",
    "    4: 'other'\n",
    "}\n",
    "\n",
    "prior_treatment_mapping = {\n",
    "    0: 'no prior treatment',\n",
    "    1: 'outpatient psychotherapy',\n",
    "    2: 'inpatient or partial inpatient treatment/psychotherapy',\n",
    "    3: 'both',\n",
    "    4: 'yes'\n",
    "}\n",
    "\n",
    "ema_smartphone_mapping = {\n",
    "    1: 'iPhone',\n",
    "    0: 'Android'\n",
    "}\n",
    "\n",
    "ema_special_event_mapping = {\n",
    "    0: 'usual',\n",
    "    1: 'special event'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply mappings\n",
    "df_redcap['gender_description'] = df_redcap['gender'].map(gender_mapping)\n",
    "df_redcap['scid_cv_description'] = df_redcap['scid_cv_prim_cat'].map(scid_cv_cat_mapping)\n",
    "df_redcap['marital_status_description'] = df_redcap['marital_status'].map(marital_status_mapping)\n",
    "df_redcap['employability_description'] = df_redcap['employability'].map(employability_mapping)\n",
    "df_redcap['graduation_description'] = df_redcap['graduation'].map(graduation_mapping)\n",
    "df_redcap['profession_description'] = df_redcap['profession'].map(profession_mapping)\n",
    "df_redcap['prior_treatment_description'] = df_redcap['prior_treatment'].map(prior_treatment_mapping)\n",
    "df_redcap['ema_smartphone_description'] = df_redcap['ema_smartphone'].map(ema_smartphone_mapping)\n",
    "df_redcap['ema_special_event_description'] = df_redcap['ema_special_event'].map(ema_special_event_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_redcap.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monitoring = df_monitoring.copy()\n",
    "df_monitoring.rename(columns = {\"Pseudonym\": \"customer\", \"EMA_ID\": \"ema_id\", \"Status\": \"status\",\n",
    "                                \"Studienversion\":\"study_version\", \"FOR_ID\":\"for_id\", \n",
    "                           \"Start EMA Baseline\": \"ema_base_start\", \"Ende EMA Baseline\": \"ema_base_end\", \n",
    "                           \"Freischaltung/ Start EMA T20\": \"ema_t20_start\",\"Ende EMA T20\":\"ema_t20_end\", \n",
    "                               \"Termin 1. GesprÃ¤ch\": \"first_call_date\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monitoring[\"customer\"] = df_monitoring[\"customer\"].str[:4]\n",
    "df_monitoring = df_monitoring[[\"customer\", \"ema_id\", \"study_version\", \"for_id\", \"status\"]]\n",
    "df_monitoring[\"for_id\"] = df_monitoring.for_id.str.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monitoring = pd.merge(df_monitoring, df_redcap, on=\"for_id\", how=\"left\")\n",
    "df_monitoring = pd.merge(df_monitoring, df_redcap_kosten, on=\"for_id\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.1 Import epoch level passive + GPS data\n",
    "\n",
    "df_complete[\"customer\"] = df_complete.customer.str.split(\"@\").str.get(0)\n",
    "df_complete[\"customer\"] = df_complete[\"customer\"].str[:4]\n",
    "\n",
    "df_complete[\"startTimestamp\"] = pd.to_datetime(df_complete[\"startTimestamp\"],unit='ms')\n",
    "df_complete[\"createdAt\"] = pd.to_datetime(df_complete[\"createdAt\"],unit='ms')\n",
    "\n",
    "## Timezone offset miteinbeziehen?\n",
    "\n",
    "# Handle NaN in timezoneOffset by skipping or replacing with a default\n",
    "# Here, we skip adjustments for NaN offsets by using np.where to check for NaN\n",
    "df_complete['startTimestamp'] = np.where(df_complete['timezoneOffset'].isna(),\n",
    "                               df_complete['startTimestamp'],  # If NaN, keep original timestamp\n",
    "                               df_complete['startTimestamp'] - pd.to_timedelta(df_complete['timezoneOffset'], unit='m'))  # Else, apply offset\n",
    "\n",
    "df_complete['createdAt'] = np.where(df_complete['createdAt'].isna(),\n",
    "                               df_complete['createdAt'],  # If NaN, keep original timestamp\n",
    "                               df_complete['createdAt'] - pd.to_timedelta(df_complete['timezoneOffset'], unit='m'))  # Else, apply offset\n",
    "\n",
    "\n",
    "df_complete[\"startTimestamp_day\"] = df_complete.startTimestamp.dt.normalize()\n",
    "df_complete[\"createdAt_day\"] = df_complete.startTimestamp.dt.normalize()\n",
    "\n",
    "df_complete[\"startTimestamp_hour\"] = df_complete.startTimestamp.dt.hour\n",
    "df_complete[\"createdAt_hour\"] = df_complete.startTimestamp.dt.hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loc_complete = df_complete[df_complete.type.isin([\"Latitude\", \"Longitude\"])]\n",
    "df_loc_complete = df_loc_complete[[\"customer\", \"startTimestamp\", \"type\", \"doubleValue\", \n",
    "                           'createdAt', 'startTimestamp_day', 'createdAt_day',\n",
    "       'startTimestamp_hour', 'createdAt_hour']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the earliest 'startTimestamp_day' for each customer\n",
    "earliest_timestamp_per_customer = df_loc_complete.groupby('customer')['startTimestamp_day'].min()\n",
    "\n",
    "# Map the earliest timestamp back to the original dataframe\n",
    "df_loc_complete['earliest_start_day'] = df_loc_complete['customer'].map(earliest_timestamp_per_customer)\n",
    "\n",
    "# Calculate 'relative_day' as the difference in days from the earliest day\n",
    "df_loc_complete['relative_day'] = (df_loc_complete['startTimestamp_day'] - df_loc_complete['earliest_start_day']).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = pd.to_datetime('today').normalize()\n",
    "df_loc_complete['potential_days_coverage'] = (today - df_loc_complete['earliest_start_day']).dt.days\n",
    "\n",
    "# Count unique days with data for each customer\n",
    "actual_days = df_loc_complete.groupby('customer')['startTimestamp_day'].nunique()\n",
    "\n",
    "# Mapping the actual number of days back to the DataFrame\n",
    "df_loc_complete['actual_days_with_data'] = df_loc_complete['customer'].map(actual_days)\n",
    "\n",
    "df_loc_complete['data_coverage_per'] = (df_loc_complete['actual_days_with_data'] / df_loc_complete['potential_days_coverage']) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd_complete = df_complete[~df_complete.type.isin([\"Latitude\", \"Longitude\"])]\n",
    "df_pd_complete = df_pd_complete[[\"customer\", \"startTimestamp\", \"type\", \"doubleValue\", \n",
    "                           \"timezoneOffset\", 'createdAt', 'startTimestamp_day', 'createdAt_day',\n",
    "       'startTimestamp_hour', 'createdAt_hour']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the earliest 'startTimestamp_day' for each customer\n",
    "earliest_timestamp_per_customer = df_pd_complete.groupby('customer')['startTimestamp_day'].min()\n",
    "\n",
    "# Map the earliest timestamp back to the original dataframe\n",
    "df_pd_complete['earliest_start_day'] = df_pd_complete['customer'].map(earliest_timestamp_per_customer)\n",
    "\n",
    "# Calculate 'relative_day' as the difference in days from the earliest day\n",
    "df_pd_complete['relative_day'] = (df_pd_complete['startTimestamp_day'] - df_pd_complete['earliest_start_day']).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd_complete['potential_days_coverage'] = (today - df_pd_complete['earliest_start_day']).dt.days\n",
    "\n",
    "# Count unique days with data for each customer\n",
    "actual_days = df_pd_complete.groupby('customer')['startTimestamp_day'].nunique()\n",
    "\n",
    "# Mapping the actual number of days back to the DataFrame\n",
    "df_pd_complete['actual_days_with_data'] = df_pd_complete['customer'].map(actual_days)\n",
    "\n",
    "df_pd_complete['data_coverage_per'] = (df_pd_complete['actual_days_with_data'] / df_pd_complete['potential_days_coverage']) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = \"15042024\"\n",
    "\n",
    "with open(datapath + f'/passive_data_{today}.pkl', 'wb') as file:\n",
    "    pickle.dump(df_pd_complete, file)\n",
    "    \n",
    "with open(datapath + f'/gps_data_{today}.pkl', 'wb') as file:\n",
    "    pickle.dump(df_loc_complete, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "session = pd.read_csv(datapath1 + \"questionnaireSession.csv\")\n",
    "\n",
    "# session data\n",
    "session[\"user\"] = session[\"user\"].str[:4]\n",
    "session.rename(columns = {\"user\":\"customer\",\"completedAt\": \"quest_complete\", \"createdAt\": \"quest_create\", \"expirationTimestamp\": \"quest_expir\"}, inplace=True)\n",
    "session[\"quest_create\"] = (pd.to_datetime(session[\"quest_create\"],unit='ms'))\n",
    "session[\"quest_complete\"] = (pd.to_datetime(session[\"quest_complete\"],unit='ms'))\n",
    "df_sess = session[[\"customer\", \"sessionRun\", \"quest_create\", \"quest_complete\", \"study\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess = pd.merge(df_sess, df_monitoring, on = \"customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess = df_sess.copy()\n",
    "df_sess[\"quest_create_day\"] = df_sess.quest_create.dt.normalize()\n",
    "df_sess[\"quest_complete_day\"] = df_sess.quest_complete.dt.normalize()\n",
    "\n",
    "df_sess[\"quest_create_hour\"] = df_sess.quest_create.dt.hour\n",
    "df_sess[\"quest_complete_hour\"] = df_sess.quest_complete.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of completed EMA beeps in first phase\n",
    "df_sess1 = df_sess.loc[df_sess.study.isin([24,25])]\n",
    "df_sess1 = df_sess1.copy()\n",
    "\n",
    "# Convert the 'ema_base_start' and 'quest_complete_day' columns to datetime\n",
    "try:\n",
    "    df_sess1['ema_start_date'] = pd.to_datetime(df_sess1['ema_start_date'])\n",
    "    df_sess1['quest_complete_day'] = pd.to_datetime(df_sess1['quest_complete_day'])\n",
    "except Exception as e:\n",
    "    print(f\"Error converting to datetime: {e}\")\n",
    "\n",
    "# Check if conversion was successful\n",
    "if df_sess1['ema_start_date'].dtype == 'datetime64[ns]' and df_sess1['quest_complete_day'].dtype == 'datetime64[ns]':\n",
    "    # Calculate 'day_relative'\n",
    "    df_sess1['quest_complete_relative1'] = (df_sess1['quest_complete_day'] - df_sess1['ema_start_date']).dt.days\n",
    "else:\n",
    "    print(\"Failed to convert one or more date columns to datetime format.\")\n",
    "\n",
    "sess_count1 = df_sess1.dropna(subset=[\"quest_complete\"]).groupby(\"customer\")[\"quest_complete\"].size()\\\n",
    ".reset_index()\n",
    "sess_count1 = sess_count1.rename(columns = {\"quest_complete\":\"nquest_EMA1\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_counts = df_sess1.groupby(['customer', 'quest_complete_day','quest_complete_relative1']).size().reset_index(name='daily_entries_sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of completed EMA beeps in second phase\n",
    "df_sess2 = df_sess.loc[df_sess.study.isin([33,34])]\n",
    "sess_count2 = df_sess2.dropna(subset=[\"quest_complete\"]).groupby(\"customer\")[\"quest_complete\"].size()\\\n",
    ".reset_index()\n",
    "sess_count2 = sess_count2.rename(columns = {\"quest_complete\":\"nquest_EMA2\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess = df_sess.merge(sess_count1, on=['customer'], how='left')\n",
    "df_sess = df_sess.merge(sess_count2, on=['customer'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess = df_sess.merge(daily_counts, on=['customer', \"quest_complete_day\"], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_df_sess = df_sess[(df_sess['quest_complete_relative1'] >= 16) | (df_sess['quest_complete_relative1'] < 0)]\n",
    "\n",
    "# Find the index of the maximum 'quest_complete_relative1' for each customer\n",
    "idx = filtered_df_sess.groupby('customer')['quest_complete_relative1'].idxmax()\n",
    "\n",
    "# Select the rows that correspond to the maximum 'quest_complete_relative1' for each customer\n",
    "max_quest_complete_relative1 = filtered_df_sess.loc[idx]\n",
    "\n",
    "# Display the rows with relevant columns\n",
    "max_quest_complete_relative1[['customer', 'for_id', 'quest_complete_relative1', 'quest_complete_day', 'quest_create_day','ema_start_date']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = \"15042024\"\n",
    "\n",
    "with open(datapath + f'/ema_data_{today}.pkl', 'wb') as file:\n",
    "    pickle.dump(df_sess, file)\n",
    "    \n",
    "with open(datapath + f'/monitoring_data_{today}.pkl', 'wb') as file:\n",
    "    pickle.dump(df_monitoring, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
